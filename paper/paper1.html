---
layout: paper
title: S2PRM
---
<body class="theme-base-0c">
    <div class="mainContent">
        <div class="divcontent">
            <div class="maintitle">
                S2PRM: Providing Crucial Reward for LLM Reasoning by Soft-step Process Reward Model
                \begin{table}[htbp]
                \centering
                \begin{tabular}{ccc c}  % 去掉竖线，使用适当的对齐方式
                \toprule
                \textbf{Base Model} & \textbf{Train} & \textbf{Test} & \textbf{Bo64 / PGI} \\ 
                \midrule
                % \multirow{4}{*}{Llama}    & M   & GSM8k        & 90.45 / 79.53 \\ 
                %                           & M+C & GSM8k        & 89.61 / \yuliang{79.53-search} \\ 
                %                           & M   & MATH500      & 41.20 / 28.60 \\ 
                %                           & M+C & MATH500      & 41.40 / \yuliang{28.00-search} \\ 
                % \midrule
                \multirow{4}{*}{Mistral}  & M   & GSM8k        & 85.82 / 77.33 \\ 
                                          & M+C & GSM8k        & 86.35 / 77.79  \\ 
                                          & M   & MATH500      & 34.80 / 26.80 \\ 
                                          & M+C & MATH500      & 35.40 / 29.00   \\ 
                \midrule
                \multirow{4}{*}{Deepseek} & C   & LeetCoTE     & 37.71 / 28.00 \\ 
                                          & C+M & LeetCoTE     & 38.86  / \yuliang{ing-1} \\ 
                                          & C   & LCB          & 25.53 / 19.92 \\ 
                                          & C+M & LCB          & \yuliang{24.96} / 20.33 \\ 
                \bottomrule
                \end{tabular}
                \caption{This table presents the test results of whether the PRMs are trained with a Mixed training dataset. When the base model is Llama or Mistral, the \textit{M+C} training data consists of the corresponding MetaMATH model-generated Mathematics dataset and full Code training dataset. When the base model is Deepseek, the \textit{M+C} training data includes all of the Code dataset and an equal amount of randomly sampled Mathematics training data. The Results column shows the Best of 64 test results.}
                \label{tab:generalization_mix}
                \end{table}
            </div>
            <div class="title">
                Abstract
            </div>
            <div class="papercontent">
                <div class="text">
                Process Reward Models (PRMs) have emerged as
                an effective method to enhance the multi-step reasoning abilities of large language models (LLMs).
                Existing PRMs typically divide the response into
                multiple reasoning steps using pre-defined placeholder tokens. However, this approach ignores
                that the true decision points are usually not associated with specific words or markers in text.
                Accordingly, we propose a model-defined reasoning step dividing method, which contrasts with
                human-defined hard division strategies, with the
                trained PRM named S2PRM (Soft-step Process
                Reward Model) This approach not only enables
                precise process rewards at model-critical reasoning positions but also improves sample efficiency
                and reduces PRMs’ training costs. Furthermore,
                this method is low-cost and can easily be extended
                to other reasoning tasks, such as code generation.
                Our experimental results demonstrate that our
                model achieves state-of-the-art Best-of-N performance. Additionally, we provide a comprehensive
                analysis and case study on both the performance
                and generalization capability of our method. For
                ease of reimplementation, our checkpoints and all
                data are available at <a class="link" href="#">GitHub Release</a>
                </div>
            </div>
        </div>
        <div class="divcontent">
            <div class="title">
                Method Overview
            </div>
            <div class="papercontent">
                <div class="text">
                Large language models (LLMs) have shown exceptional
                performance across various tasks. Nonetheless, even stateof-the-art LLMs encounter difficulties when addressing complex, multi-step reasoning problems, such as mathematical
                reasoning and code reasoning tasks (Huang et al., 2024;
                Tyen et al., 2024; Mirzadeh et al., 2024). One effective
                method involves training reward models to judge the correctness of model completions. With a good reward model,we can adopt reinforcement learning from the human feedback (RLHF) method, rejection sampling method, or Best
                of N method to further enhance the precision and reliability
                of the LLM outputs (Shao et al., 2024; Sessa et al., 2024;
                Gao et al., 2024).
                </div>
            </div>
            <div class="papercontent">
                <embed src="../picture/figure-intro-main.pdf" style="width: 80%;height: 400px;">
                    <div class="text">
                    Figure 1: Suggested by (Kahneman, 2011), the human thinking process requires different reasoning costs depending on
                    task difficulty. Simple problems can be quickly resolved,
                    while complex problems require more effort at certain decision points, which we term <strong>decision point</strong>  in the reasoning
                    process.
                    </div>
            </div>
        </div>
        <div class="divcontent">
            <div class="title">
                Introduction
            </div>
            <div class="papercontent">
                <div class="text">

                </div>
            </div>

            <div class="title">
                Method
            </div>
            <div class="papercontent">
                <div class="text">

                </div>
            </div>
        </div>
        <div class="divcontent">
            <div class="title">
                Experiment Results
            </div>
        </div>
        
    </div>
</body>

<link rel="stylesheet" href="../css/content.css">
